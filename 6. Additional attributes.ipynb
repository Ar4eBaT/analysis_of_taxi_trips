{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Дополнительные признаки\n",
    "\n",
    "На этой неделе вам предстоит попробовать добавить в вашу регрессионную модель дополнительные признаки.\n",
    "\n",
    "Во-первых, для прогнозирования можно использовать информацию, содержащуюся в сырых данных:\n",
    "\n",
    "- средняя длительность поездок\n",
    "- среднее количество пассажиров\n",
    "- среднее расстояние по счётчику\n",
    "- доли географических зон, в которые совершаются поездки\n",
    "- доли поездок, совершаемых по тарифам каждого из типов\n",
    "- доли способов оплаты поездок\n",
    "- средняя стоимость поездок\n",
    "- доли провайдеров данных\n",
    "\n",
    "Все эти признаки можно использовать только с задержкой, то есть, при прогнозировании $\\hat{y}_{T+i|T}$ эти признаки должны быть рассчитаны по данным не позднее момента времени $T$. Каждый из этих признаков можно использовать по-разному: как сырые значения за последние несколько часов, так и средние за последний день, неделю, месяц и т. д.\n",
    "\n",
    "Во-вторых, чтобы улучшить качество прогнозов в аномальные периоды, вы можете найти информацию о потенциально влияющих на количество поездок событиях, таких, как государственные праздники. Проанализируйте, как именно поведение пассажиров меняется во время этих событий, и создайте признаки, отражающие эти изменения. Как показывает наш опыт, правильный учёт праздничных дней часто позволяет существенно уменьшить среднюю ошибку прогноза.\n",
    "\n",
    "В-третьих, можно использовать признаки, связанные с географией. Например, скорее всего, суммарное количество поездок, совершаемых из географической зоны, пропорционально площади этой зоны. Для зон, прилегающих к аэропорту, может быть характерен специфический паттерн дневной сезонности, связанный с тем, что спрос на такси будет повышаться в те часы, когда общественный транспорт перестаёт работать. В деловом центре максимальное количество поездок будет приходиться на начало и окончание рабочего дня, на Бродвее — на время начала и окончания спектаклей. Все эти идеи не обязательно верны, мы приводим их здесь только для того, чтобы продемонстрировать принцип рассуждений. Ещё один пример географического признака: можно попробовать добавить идентификатор боро, который можно найти в файле https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv. Кроме того, нам кажется перспективным использование в качестве фактора количества поездок, совершённых за прошлый час/день и т. д. из соседних географических зон, или количества поездок, совершённых за прошлый час/день в текущую географическую зону.\n",
    "\n",
    "Много примеров других признаков, которые можно использовать при регрессионном прогнозировании, можно найти в [лекции](https://habrahabr.ru/company/yandex/blog/316232) Вадима Стрижова.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 1\n",
    "\n",
    "Попробуйте добавить признаки. Используйте идеи, которые мы предложили, или какие-то свои. Обучайте обновлённые модели на данных до апреля 2016 включительно и считайте качество новых прогнозов на мае. Удаётся ли вам улучшить качество? Не нужно ли увеличить сложность регрессионной модели? Если добавляемый признак не улучшает качество, всё равно оставьте доказательства этому в ноутбуке, чтобы ваши коллеги это видели при проверке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция разделения единого массива данных на обучающую, тестовую и контрольную выборки\n",
    "\n",
    "def split_data(input_data):\n",
    "    train_data = pd.concat([input_data[input_data.year < 2016],\n",
    "                            input_data[(input_data.year == 2016) & (input_data.month.astype(int) < 5)]])\n",
    "\n",
    "    test_data = input_data[(input_data.year == 2016) & (input_data.month == \"5\")]\n",
    "    control_data = pd.concat([test_data[-102:],\n",
    "                             input_data[(input_data.year == 2016) & (input_data.month == \"6\")]])\n",
    "    test_data = test_data[:-102]\n",
    "    \n",
    "    return train_data, test_data, control_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = np.sort(glob(\"new_data/*.csv\", recursive=True))\n",
    "input_data = []\n",
    "for path in file_path:\n",
    "    new_data = pd.read_csv(path)\n",
    "    new_data.month = new_data.month.astype(str)\n",
    "    input_data.append(new_data)\n",
    "    \n",
    "result_data = []\n",
    "for i in range(len(input_data)):\n",
    "    i_data = input_data[i]\n",
    "    result_data.append(split_data(i_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция считывания оргинальных данных\n",
    "def read_data(name, state):\n",
    "    columns = []\n",
    "    if state == \"bad\":\n",
    "        columns = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\",\n",
    "               \"trip_distance\", \"RatecodeID\", \"store_and_fwd_flag\", \"PULocationID\", \"DOLocationID\",\n",
    "               \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\",\n",
    "               \"improvement_surcharge\", \"total_amount\", \"\", \"\"]\n",
    "    else:\n",
    "        columns = [\"vendor_id\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\",\n",
    "               \"trip_distance\", \"pickup_longitude\", 'pickup_latitude', \"rate_code\", \"store_and_fwd_flag\",\n",
    "               \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
    "               \"surcharge\", \"mta_tax\", 'tip_amount', \"tolls_amount\", \"total_amount\"]\n",
    "        \n",
    "    parse_dates = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "    data = {\n",
    "    'normal': lambda name: pd.read_csv(\"origin_data/\" + name, header=0,\n",
    "                   sep=\",\", parse_dates=parse_dates),\n",
    "    'bad': lambda name: pd.read_csv(\"origin_data/\" + name, sep=\",\", names=columns,\n",
    "                   skiprows=1, parse_dates=parse_dates),\n",
    "    'old': lambda name: pd.read_csv(\"origin_data/\" + name, skiprows=1,\n",
    "                   sep=\",\", parse_dates=parse_dates, names=columns)\n",
    "    }[state](name)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Функция построения массива дат на заданном интервале\n",
    "\n",
    "def daterange(date1, date2, move=5):\n",
    "    dates = []\n",
    "    for n in range(int((date2 - date1).days * 24) + 24 - move):\n",
    "        dates.append(date1 + timedelta(n / 24))\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_regions = pd.unique(result_data[0][2][\"region\"])\n",
    "\n",
    "# Функция считывания оргинальных данных с предварительным аггрегированием\n",
    "\n",
    "def open_data(file_path):\n",
    "    data = read_data(file_path, \"normal\")\n",
    "\n",
    "    data = data[data.tpep_pickup_datetime != data.tpep_dropoff_datetime]\n",
    "    data = data[data.passenger_count != 0]\n",
    "    data = data[data.trip_distance != 0.0]\n",
    "    \n",
    "    region_data = pd.read_csv(\"regions.csv\", sep=\";\")\n",
    "    \n",
    "    regions = []\n",
    "    for region in region_data.get_values():\n",
    "        region_id = int(region[0])\n",
    "        index = data[(data[\"pickup_longitude\"].between(region[1], region[2])) & (data[\"pickup_latitude\"].between(region[3], region[4]))].index\n",
    "        region_vals = np.repeat(\"region_\" + str(region_id), index.shape[0])\n",
    "        regions.append(pd.DataFrame(region_vals, columns=[\"region\"], index=index))\n",
    "    \n",
    "    regions = pd.concat(regions).sort_index()\n",
    "    regions = regions[regions.index.duplicated() == False]\n",
    "    data = data[data.index.isin(regions.sort_index().index)]\n",
    "    \n",
    "    columns = [\"VendorID\", \"tpep_pickup_datetime\", \"passenger_count\", \"trip_distance\",\n",
    "               \"store_and_fwd_flag\", \"payment_type\", \"total_amount\"]\n",
    "    \n",
    "    data = pd.concat([data[columns], regions], axis=1) \n",
    "    data = data[data.region.isin(most_regions)]\n",
    "    \n",
    "    data.tpep_pickup_datetime = list(map(lambda x: x.replace(minute=0, second=0), data.tpep_pickup_datetime))\n",
    "    data.store_and_fwd_flag = data.store_and_fwd_flag.fillna(\"UN\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция преобразования признаков\n",
    "\n",
    "def get_aggregate_region(data, region):\n",
    "    dates = daterange(data.tpep_pickup_datetime.min(), data.tpep_pickup_datetime.max(), 0)\n",
    "    res = pd.DataFrame({\"region\" : np.repeat(region, len(dates)), \"time\" : dates})\n",
    "    \n",
    "    cmt_res = data[(data.VendorID == 1) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    cmt_res = pd.DataFrame({\"time\" : cmt_res.index.get_values(),\n",
    "                            \"CMT_vendor\" : cmt_res[\"VendorID\"].get_values()})\n",
    "\n",
    "    res = pd.merge(res, cmt_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    vts_res = data[(data.VendorID == 2) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    vts_res = pd.DataFrame({\"time\" : vts_res.index.get_values(),\n",
    "                            \"VTS_vendor\" : vts_res[\"VendorID\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, vts_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    mean_res = data[data.region == region].groupby(\"tpep_pickup_datetime\").mean()\n",
    "    mean_res = pd.DataFrame({\"time\" : mean_res.index.get_values(),\n",
    "                             \"passenger_count\" : mean_res[\"passenger_count\"].get_values(),\n",
    "                             \"trip_distance\" : mean_res[\"trip_distance\"].get_values(),\n",
    "                             \"total_amount\" : mean_res[\"total_amount\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, mean_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    csh_res = data[(data.payment_type == 2) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    csh_res = pd.DataFrame({\"time\" : csh_res.index.get_values(),\n",
    "                            \"CSH_payment_type\" : csh_res[\"payment_type\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, csh_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    crd_res = data[(data.payment_type == 1) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    crd_res = pd.DataFrame({\"time\" : crd_res.index.get_values(),\n",
    "                            \"CRD_payment_type\" : crd_res[\"payment_type\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, crd_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    dis_res = data[(data.payment_type == 4) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    dis_res = pd.DataFrame({\"time\" : dis_res.index.get_values(),\n",
    "                            \"DIS_payment_type\" : dis_res[\"payment_type\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, dis_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    noc_res = data[(data.payment_type == 3) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    noc_res = pd.DataFrame({\"time\" : noc_res.index.get_values(),\n",
    "                            \"NOC_payment_type\" : noc_res[\"payment_type\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, noc_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    unk_res = data[(data.payment_type == 5) & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    unk_res = pd.DataFrame({\"time\" : unk_res.index.get_values(),\n",
    "                            \"UNK_payment_type\" : unk_res[\"payment_type\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, unk_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    n_res = data[(data.store_and_fwd_flag == \"N\") & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    n_res = pd.DataFrame({\"time\" : n_res.index.get_values(),\n",
    "                          \"N_store_and_fwd_flag\" : n_res[\"store_and_fwd_flag\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, n_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    un_res = data[(data.store_and_fwd_flag == \"UN\") & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    un_res = pd.DataFrame({\"time\" : un_res.index.get_values(),\n",
    "                           \"UN_store_and_fwd_flag\" : un_res[\"store_and_fwd_flag\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, un_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    y_res = data[(data.store_and_fwd_flag == \"Y\") & (data.region == region)].groupby(\"tpep_pickup_datetime\").count()\n",
    "    y_res = pd.DataFrame({\"time\" : y_res.index.get_values(),\n",
    "                          \"Y_store_and_fwd_flag\" : y_res[\"store_and_fwd_flag\"].get_values()})\n",
    "    \n",
    "    res = pd.merge(res, y_res, how='left', on=\"time\").fillna(0)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Функция подготовки данных для слияния с исходной выборкой\n",
    "\n",
    "def prepare_data(file_name):\n",
    "    data = open_data(file_name)\n",
    "    \n",
    "    trans_res = []\n",
    "    for region in most_regions:\n",
    "        trans_res.append(get_aggregate_region(data, region))\n",
    "    \n",
    "    res_data = pd.concat(trans_res)\n",
    "    res_data.to_csv(\"trans_data/\" + file_name, index=False)\n",
    "    \n",
    "    os.remove(\"origin_data/\" + file_name)\n",
    "    \n",
    "    del data\n",
    "    del res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin!\n",
      "yellow_tripdata_2015-05.csv\n",
      "yellow_tripdata_2015-11.csv\n",
      "yellow_tripdata_2015-10.csv\n",
      "yellow_tripdata_2015-04.csv\n",
      "yellow_tripdata_2015-12.csv\n",
      "yellow_tripdata_2015-06.csv\n",
      "yellow_tripdata_2015-07.csv\n",
      "yellow_tripdata_2015-03.csv\n",
      "yellow_tripdata_2015-02.csv\n",
      "yellow_tripdata_2015-01.csv\n",
      "yellow_tripdata_2016-02.csv\n",
      "yellow_tripdata_2016-03.csv\n",
      "yellow_tripdata_2016-01.csv\n",
      "yellow_tripdata_2016-04.csv\n",
      "yellow_tripdata_2016-05.csv\n",
      "yellow_tripdata_2016-06.csv\n",
      "yellow_tripdata_2015-09.csv\n",
      "yellow_tripdata_2015-08.csv\n",
      "Done!\n",
      "CPU times: user 8h 14min 58s, sys: 30min 11s, total: 8h 45min 10s\n",
      "Wall time: 8h 47min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_list = os.listdir(\"origin_data\")\n",
    "\n",
    "print(\"Begin!\")\n",
    "for file_name in path_list:\n",
    "    print(file_name)\n",
    "    prepare_data(file_name)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе происходит подготовка данных с сайта открытых данных Нью Йорка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ar4eBaT/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (10,11,14,15,20,23,25,40,84,85,86,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/Ar4eBaT/anaconda/lib/python3.6/site-packages/pandas/core/internals.py:3462: FutureWarning: Passing in 'datetime64' dtype with no frequency is deprecated and will raise in a future version. Please pass in 'datetime64[ns]' instead.\n",
      "  return self.apply('astype', dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>visibility</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew_point_temp</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  visibility  temp  dew_point_temp  relative_humidity  \\\n",
       "0 2013-01-01 00:00:00        10.0   0.8            -4.4               55.0   \n",
       "1 2013-01-01 01:00:00        10.0   0.8            -3.3               62.0   \n",
       "2 2013-01-01 02:00:00        10.0   1.3            -2.8               62.0   \n",
       "3 2013-01-01 03:00:00        10.0   1.3            -2.8               62.0   \n",
       "4 2013-01-01 04:00:00        10.0   1.3            -2.8               62.0   \n",
       "\n",
       "   wind_speed  \n",
       "0        10.0  \n",
       "1         3.0  \n",
       "2         7.0  \n",
       "3         9.0  \n",
       "4         7.0  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"time\", \"visibility\", \"temp\", \"dew_point_temp\",\n",
    "           \"relative_humidity\", \"wind_speed\"]\n",
    "\n",
    "weather_data = pd.read_csv(\"weather_data.csv\", date_parser=[\"DATE\"])\n",
    "\n",
    "weather_data[\"time\"] = list(map(lambda x: x.replace(minute=0, second=0), weather_data.DATE.astype(np.datetime64)))\n",
    "weather_data[\"visibility\"] = list(map(lambda x: float(str(x).split(\"V\")[0]),weather_data.HOURLYVISIBILITY))\n",
    "weather_data[\"temp\"] = weather_data.HOURLYWETBULBTEMPC\n",
    "weather_data[\"dew_point_temp\"] = list(map(lambda x: float(str(x).split(\"s\")[0]),weather_data.HOURLYDewPointTempC))\n",
    "weather_data[\"relative_humidity\"] = weather_data.HOURLYRelativeHumidity\n",
    "weather_data[\"wind_speed\"] = weather_data.HOURLYWindSpeed\n",
    "\n",
    "weather_data = weather_data[weather_data.time.duplicated() == False]\n",
    "weather_data = weather_data[columns]\n",
    "weather_data = weather_data.fillna(0.0)\n",
    "weather_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе определяются все государственные праздники"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday                time\n",
       "0     True 2013-01-01 00:00:00\n",
       "1     True 2013-01-01 01:00:00\n",
       "2     True 2013-01-01 02:00:00\n",
       "3     True 2013-01-01 03:00:00\n",
       "4     True 2013-01-01 04:00:00"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holidays = [datetime(2013, 1, 1), datetime(2013, 1, 21), datetime(2013, 2, 18), datetime(2013, 5, 27),\n",
    "            datetime(2013, 7, 4), datetime(2013, 9, 2), datetime(2013, 10, 14), datetime(2013, 11, 11),\n",
    "           datetime(2013, 11, 28), datetime(2013, 12, 25), datetime(2014, 1, 1), datetime(2014, 1, 20),\n",
    "            datetime(2014, 2, 17), datetime(2014, 5, 26), datetime(2014, 7, 4), datetime(2014, 9, 1),\n",
    "            datetime(2014, 10, 13), datetime(2014, 11, 11), datetime(2014, 11, 27), datetime(2014, 12, 25),\n",
    "           datetime(2015, 1, 1), datetime(2015, 1, 19), datetime(2015, 2, 16), datetime(2015, 5, 25),\n",
    "            datetime(2015, 7, 3), datetime(2015, 9, 7), datetime(2015, 10, 12), datetime(2015, 11, 11),\n",
    "            datetime(2015, 11, 26), datetime(2015, 12, 25), datetime(2016, 1, 1), datetime(2016, 1, 18),\n",
    "            datetime(2016, 2, 15), datetime(2016, 5, 30)]\n",
    "dates = []\n",
    "for holiday in holidays:\n",
    "    dates.extend(daterange(holiday, holiday, 0))\n",
    "    \n",
    "holidays_data = pd.DataFrame({\"time\":dates, \"holiday\": np.repeat(True, len(dates))})\n",
    "holidays_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>visibility</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew_point_temp</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  visibility  temp  dew_point_temp  relative_humidity  \\\n",
       "0 2013-01-01 00:00:00        10.0   0.8            -4.4               55.0   \n",
       "1 2013-01-01 01:00:00        10.0   0.8            -3.3               62.0   \n",
       "2 2013-01-01 02:00:00        10.0   1.3            -2.8               62.0   \n",
       "3 2013-01-01 03:00:00        10.0   1.3            -2.8               62.0   \n",
       "4 2013-01-01 04:00:00        10.0   1.3            -2.8               62.0   \n",
       "\n",
       "   wind_speed  holiday  \n",
       "0        10.0     True  \n",
       "1         3.0     True  \n",
       "2         7.0     True  \n",
       "3         9.0     True  \n",
       "4         7.0     True  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_data = pd.merge(weather_data, holidays_data, how=\"left\").fillna(False)\n",
    "attr_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ar4eBaT/anaconda/lib/python3.6/site-packages/pandas/core/internals.py:3462: FutureWarning: Passing in 'datetime64' dtype with no frequency is deprecated and will raise in a future version. Please pass in 'datetime64[ns]' instead.\n",
      "  return self.apply('astype', dtype=dtype, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "path_list = os.listdir(\"trans_data\")\n",
    "\n",
    "data_list = []\n",
    "for path in path_list:\n",
    "    data_list.append(pd.read_csv(\"trans_data/\"+path, date_parser=[\"time\"]))\n",
    "    \n",
    "trans_data = pd.concat(data_list)\n",
    "trans_data.time = trans_data.time.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>time</th>\n",
       "      <th>CMT_vendor</th>\n",
       "      <th>VTS_vendor</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>CSH_payment_type</th>\n",
       "      <th>CRD_payment_type</th>\n",
       "      <th>DIS_payment_type</th>\n",
       "      <th>...</th>\n",
       "      <th>UNK_payment_type</th>\n",
       "      <th>N_store_and_fwd_flag</th>\n",
       "      <th>UN_store_and_fwd_flag</th>\n",
       "      <th>Y_store_and_fwd_flag</th>\n",
       "      <th>visibility</th>\n",
       "      <th>temp</th>\n",
       "      <th>dew_point_temp</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1449384</th>\n",
       "      <td>region_1280</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.713333</td>\n",
       "      <td>14.699533</td>\n",
       "      <td>3.047933</td>\n",
       "      <td>69.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486584</th>\n",
       "      <td>region_1684</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444176</th>\n",
       "      <td>region_1234</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>118.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.816514</td>\n",
       "      <td>13.047431</td>\n",
       "      <td>2.910780</td>\n",
       "      <td>103.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476912</th>\n",
       "      <td>region_1437</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.212121</td>\n",
       "      <td>14.225758</td>\n",
       "      <td>3.569697</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491792</th>\n",
       "      <td>region_2119</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>49.096569</td>\n",
       "      <td>14.718725</td>\n",
       "      <td>46.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              region       time  CMT_vendor  VTS_vendor  passenger_count  \\\n",
       "1449384  region_1280 2013-03-01        70.0        80.0         1.713333   \n",
       "1486584  region_1684 2013-03-01         0.0         0.0         0.000000   \n",
       "1444176  region_1234 2013-03-01       118.0       100.0         1.816514   \n",
       "1476912  region_1437 2013-03-01        18.0        15.0         2.212121   \n",
       "1491792  region_2119 2013-03-01        45.0        57.0         1.941176   \n",
       "\n",
       "         total_amount  trip_distance  CSH_payment_type  CRD_payment_type  \\\n",
       "1449384     14.699533       3.047933              69.0              80.0   \n",
       "1486584      0.000000       0.000000               0.0               0.0   \n",
       "1444176     13.047431       2.910780             103.0             112.0   \n",
       "1476912     14.225758       3.569697              19.0              14.0   \n",
       "1491792     49.096569      14.718725              46.0              55.0   \n",
       "\n",
       "         DIS_payment_type   ...     UNK_payment_type  N_store_and_fwd_flag  \\\n",
       "1449384               0.0   ...                  1.0                  68.0   \n",
       "1486584               0.0   ...                  0.0                   0.0   \n",
       "1444176               0.0   ...                  0.0                 117.0   \n",
       "1476912               0.0   ...                  0.0                  18.0   \n",
       "1491792               0.0   ...                  0.0                  45.0   \n",
       "\n",
       "         UN_store_and_fwd_flag  Y_store_and_fwd_flag  visibility  temp  \\\n",
       "1449384                   80.0                   2.0        10.0   1.4   \n",
       "1486584                    0.0                   0.0        10.0   1.4   \n",
       "1444176                  100.0                   1.0        10.0   1.4   \n",
       "1476912                   15.0                   0.0        10.0   1.4   \n",
       "1491792                   57.0                   0.0        10.0   1.4   \n",
       "\n",
       "         dew_point_temp  relative_humidity  wind_speed  holiday  \n",
       "1449384            -1.7               70.0         7.0    False  \n",
       "1486584            -1.7               70.0         7.0    False  \n",
       "1444176            -1.7               70.0         7.0    False  \n",
       "1476912            -1.7               70.0         7.0    False  \n",
       "1491792            -1.7               70.0         7.0    False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_attr = pd.merge(trans_data, attr_data, how=\"left\", on=\"time\").sort_values(\"time\")\n",
    "complete_attr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_attr.to_csv(\"add_attributes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 2\n",
    "\n",
    "Когда вы примете решение остановиться и перестать добавлять признаки, постройте для каждой географической зоны и каждого конца истории от 2016.04.30 23:00 до 2016.05.31 17:00 прогнозы на 6 часов вперёд; посчитайте в ноутбуке ошибку прогноза по следующему функционалу:\n",
    "\n",
    "<center>$Q_{june}=\\frac{1}{R∗739∗6}\\sum_{r=1}^R\\sum_{T=2016.05.01-00:00}^{T=2016.05.31-23:00}\\sum_{i=1}^6\\mid{\\widehat{y}}^r_{T|T+i}−{\\widehat{y}}^r_{T+i}\\mid.$</center>\n",
    "\n",
    "Убедитесь, что среднее качество прогнозов увеличилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = np.sort(glob(\"complete_data/*.csv\", recursive=True))\n",
    "input_data = []\n",
    "for path in file_path:\n",
    "    new_data = pd.read_csv(path)\n",
    "    new_data.month = new_data.month.astype(str)\n",
    "    new_data.hour = new_data.hour.astype(str)\n",
    "    input_data.append(new_data)\n",
    "    \n",
    "result_data = []\n",
    "for i in range(len(input_data)):\n",
    "    i_data = input_data[i]\n",
    "    result_data.append(split_data(i_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Trend', 'cos_week_1', 'cos_week_2', 'cos_week_3', 'cos_week_4',\n",
       "       'cos_week_5', 'cos_week_6', 'cos_year_1', 'cos_year_10', 'cos_year_11',\n",
       "       ...\n",
       "       'NOC_payment_type', 'N_store_and_fwd_flag', 'UN_store_and_fwd_flag',\n",
       "       'Y_store_and_fwd_flag', 'visibility', 'temp', 'dew_point_temp',\n",
       "       'relative_humidity', 'wind_speed', 'holiday'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_cols = input_data[0].columns[(input_data[0].columns != \"trips\") & (input_data[0].columns != \"year\") & (input_data[0].columns != \"day\") & (input_data[0].columns != \"UNK_payment_type\")]\n",
    "attribute_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer as DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_attributes = [\"region\", \"month\", \"dayofweek\", \"hour\"]\n",
    "cat_attributes = [\"region\", \"month\", \"dayofweek\"]\n",
    "regions = pd.unique(result_data[0][0][\"region\"])\n",
    "month = list(map(lambda x: \"m_\" + str(x), (pd.unique(result_data[0][0][\"month\"]))))\n",
    "# hours = list(map(lambda x: \"h_\" + str(x), (pd.unique(result_data[0][0][\"hour\"]))))\n",
    "dayofweek = pd.unique(result_data[0][0][\"dayofweek\"])\n",
    "# cat_columns = np.append(np.append(np.append(regions, month), dayofweek), hours)\n",
    "cat_columns = np.append(np.append(regions, month), dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = DV(sparse = False)\n",
    "cat_fit_data = result_data[0][0][cat_attributes].T.to_dict().values()\n",
    "encoder.fit(cat_fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(attribute_cols)\n",
    "a.remove(\"month\")\n",
    "a.remove('dayofweek')\n",
    "a.remove('hour')\n",
    "a.remove('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.474860038738093\n",
      "10.189087001512378\n",
      "11.80450529332166\n",
      "12.39929422377882\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for data in result_data:\n",
    "    estimator = RandomForestRegressor()\n",
    "    \n",
    "    train_data = data[0].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    encoded_data = encoder.transform(train_data[cat_attributes].T.to_dict().values())\n",
    "    cat_att = pd.DataFrame(encoded_data, columns=cat_columns)\n",
    "    cat_att.index = train_data[a].index\n",
    "\n",
    "    X_train = pd.concat([train_data[a], cat_att], axis=1)\n",
    "    y_train = train_data[\"trips\"]\n",
    "\n",
    "    fitted = estimator.fit(X_train, y_train)\n",
    "    \n",
    "    test_data = data[1].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    encoded_data = encoder.transform(test_data[cat_attributes].T.to_dict().values())\n",
    "    cat_att = pd.DataFrame(encoded_data, columns=cat_columns)\n",
    "    cat_att.index = test_data[a].index\n",
    "\n",
    "    X_test = pd.concat([test_data[a], cat_att], axis=1)\n",
    "    y_test = test_data[\"trips\"]\n",
    "\n",
    "    prediction = fitted.predict(X_test)\n",
    "    estimate = sum(np.abs(prediction.astype(int) - y_test)) / (102 * 739)\n",
    "    \n",
    "    results.append((fitted, estimate))\n",
    "    print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = 0\n",
    "for i in range(6):\n",
    "    estimate += results[i][1]\n",
    "print(estimate / 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 3\n",
    "\n",
    "Переобучите итоговые модели на данных до мая 2016 включительно, постройте прогнозы на июнь для каждого конца истории от 2016.05.31 23:00 до 2016.06.30 17:00 и запишите все результаты в один файл в уже знакомом вам формате: geoID, histEndDay, histEndHour, step, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "control_data = []\n",
    "\n",
    "for data in result_data:\n",
    "    train_data.append(pd.concat([data[0], data[1]]))\n",
    "    control_data.append(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "CPU times: user 6h 25min 59s, sys: 19min 44s, total: 6h 45min 43s\n",
      "Wall time: 6h 50min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_names =  [\"1. first_prediction\", \"2. second_prediction\", \"3. third_prediction\",\n",
    "               \"4. forth_prediction\", \"5. fifth_prediction\", \"6. sixth_prediction\"]\n",
    "\n",
    "results = []\n",
    "for i in range(len(train_data)):\n",
    "    estimator = RandomForestRegressor()\n",
    "    \n",
    "    data = (train_data[i], control_data[i])\n",
    "    \n",
    "    t_data = data[0].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    encoded_data = encoder.transform(t_data[cat_attributes].T.to_dict().values())\n",
    "    cat_att = pd.DataFrame(encoded_data, columns=cat_columns)\n",
    "    cat_att.index = t_data[a].index\n",
    "\n",
    "    X_train = pd.concat([t_data[a], cat_att], axis=1)\n",
    "    y_train = t_data[\"trips\"]\n",
    "\n",
    "    fitted = estimator.fit(X_train, y_train)\n",
    "    \n",
    "    test_data = data[1].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    \n",
    "    encoded_data = encoder.transform(test_data[cat_attributes].T.to_dict().values())\n",
    "    cat_att = pd.DataFrame(encoded_data, columns=cat_columns)\n",
    "    cat_att.index = test_data[a].index\n",
    "\n",
    "    X_test = pd.concat([test_data[a], cat_att], axis=1)\n",
    "\n",
    "    prediction = fitted.predict(X_test)\n",
    "    \n",
    "    res_shape = test_data[\"region\"].get_values().shape[0]\n",
    "    \n",
    "    prediction = pd.DataFrame(np.concatenate((test_data[\"region\"].get_values().reshape(res_shape, 1),\n",
    "                                              abs(prediction.astype(int)).reshape(res_shape, 1)),\n",
    "                                             axis=1),\n",
    "                              columns=[\"region\", \"trips\"])\n",
    "    \n",
    "    prediction.to_csv(\"new_result/\" + file_names[i] + \".csv\", index=False)\n",
    "    \n",
    "    results.append((fitted, prediction))\n",
    "    \n",
    "    print(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dt = datetime(2016, 5, 31, 23)\n",
    "end_dt = datetime(2016, 6, 30, 17)\n",
    "dates = daterange(start_dt, end_dt)\n",
    "\n",
    "date_list = list(map(lambda date: np.repeat(date.strftime(\"_%Y-%m-%d_\") + str(date.hour) + \"_\", 102), dates))\n",
    "date_list = np.concatenate(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72930, 2)\n",
      "(72930, 2)\n",
      "(72930, 2)\n",
      "(72930, 2)\n",
      "(72930, 2)\n",
      "(72930, 2)\n"
     ]
    }
   ],
   "source": [
    "file_path = np.sort(glob(\"new_result/*.csv\", recursive=True))\n",
    "solution_data = []\n",
    "for i in range(len(file_path)):\n",
    "    path = file_path[i]\n",
    "    result = pd.read_csv(path)[:len(dates * 102)]\n",
    "    solution_data.append(result)\n",
    "    print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = []\n",
    "for i in range(6):\n",
    "    res = solution_data[i]\n",
    "    regions = list(map(lambda reg: reg.split(\"_\")[1], solution_data[i][\"region\"]))\n",
    "\n",
    "    ids = []\n",
    "    for j in range(len(date_list)):\n",
    "        ids.append(regions[j] + date_list[j])\n",
    "    \n",
    "    res[\"id\"] = list(map(lambda _id: _id + str(i+1), ids))\n",
    "    res[\"y\"] = res.trips\n",
    "    solutions.append(res[[\"id\", \"y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(solutions).sort_values(\"id\").to_csv(\"new_solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 4\n",
    "\n",
    "Загрузите полученный файл на kaggle: https://inclass.kaggle.com/c/yellowtaxi. Добавьте в ноутбук ссылку на сабмишн."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текущий результат **11.62815** (Прошлый результат **18.27950**)\n",
    "\n",
    "https://www.kaggle.com/submissions/6651636/6651636.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
